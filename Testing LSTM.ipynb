{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b47eda-b5d5-4097-89b3-8ced156bd198",
   "metadata": {},
   "source": [
    "# Note: This is a computational heavy task and might crash your browser\n",
    "# Importing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f66100a-fbd4-4b41-a973-bc548fbe1dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e05f08-733c-4c69-9587-bc3706f0861c",
   "metadata": {},
   "source": [
    "# Reading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "487d8684-ba83-4c95-bd63-123ae9e76978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_list):\n",
    "    '''\n",
    "    Reads the txt file and assigns the parameters to respective list, updating the dictionary.\n",
    "    Also, performing One Hot Encoding on the Sentiments.\n",
    "    '''\n",
    "    dataset = {}\n",
    "    for path in file_list:\n",
    "        dataset[path] = {}\n",
    "        tweet = []\n",
    "        tweetgts = []\n",
    "        tweetid = []\n",
    "        with open(path, encoding='utf8') as file:\n",
    "            for line in file:\n",
    "                line = line[:len(line) - 1]\n",
    "                contents = line.split('\\t')\n",
    "                tweetid.append(int(contents[0]))\n",
    "                if(contents[1] == 'positive'):\n",
    "                    tweetgts.append([0, 1, 0])\n",
    "                elif(contents[1] == 'negative'):\n",
    "                    tweetgts.append([0, 0, 1])\n",
    "                else:\n",
    "                    tweetgts.append([1, 0, 0])\n",
    "                tweet.append(contents[2])\n",
    "        dataset[path]['tweet'] = tweet\n",
    "        dataset[path]['sentiment'] = tweetgts\n",
    "        dataset[path]['ids'] = tweetid\n",
    "    return dataset\n",
    "dataset = read_file(['twitter-training-data.txt', 'twitter-dev-data.txt','twitter-test1.txt','twitter-test2.txt','twitter-test3.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c84691f-a54f-40b4-afcf-58002d160066",
   "metadata": {},
   "source": [
    "## LSTM will take a lot of time if running on cpu,\n",
    "## This checks if our machine has cuda cores or not.\n",
    "## Cuda can be enabled for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e6f1e7-9b5e-42a6-941e-6410ff02def1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5425d32a-5da7-4290-93e1-ca21cecc4eae",
   "metadata": {},
   "source": [
    "## TEXT Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb716d3b-ebd4-4038-9d1c-6c8135328432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(texts):\n",
    "    '''\n",
    "    Pre-processed the tweets and returns a clean tweets after\n",
    "    replacing and removing the unwanted bits and pieces from the tweet.\n",
    "    '''\n",
    "    cleaned_text = []\n",
    "    for text in texts:\n",
    "        # remove ugly &quot and &amp\n",
    "        text = re.sub(r\"&quot;(.*?)&quot;\", \"\\g<1>\", text)\n",
    "        text = re.sub(r\"&amp;\", \"\", text)\n",
    "\n",
    "        # replace emoticon\n",
    "        text = re.sub(\n",
    "            r\"(^| )(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)\",\n",
    "            \"\\g<1>TOKEMOTICON\",\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"tokemoticon\", \"TOKEMOTICON\")\n",
    "\n",
    "        # replace url\n",
    "        text = re.sub(\n",
    "            r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\",\n",
    "            \"TOKURL\",\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        # replace mention\n",
    "        text = re.sub(r\"@[\\w]+\", \"TOKMENTION\", text)\n",
    "\n",
    "        # replace hashtag\n",
    "        text = re.sub(r\"#[\\w]+\", \"TOKTAG\", text)\n",
    "\n",
    "        # replace dollar\n",
    "        text = re.sub(r\"\\Â£\\d+\", \"TOKPOUND\", text)\n",
    "\n",
    "        # remove punctuation\n",
    "        text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "\n",
    "        # remove multiple spaces\n",
    "        text = re.sub(r\" +\", \" \", text)\n",
    "\n",
    "        # remove newline\n",
    "        text = re.sub(r\"\\n\", \" \", text)\n",
    "        \n",
    "        #Remove Digits\n",
    "        text= re.sub('[0-9\\n]',' ',text)\n",
    "\n",
    "        cleaned_text.append(text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7503e8-8de4-4f35-8e9f-0a808e0c37b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35402\n"
     ]
    }
   ],
   "source": [
    "cleaned_tweets = cleanup_text(dataset['twitter-training-data.txt']['tweet'])\n",
    "tokenizer = Tokenizer(num_words = 5000,oov_token='<oov>')\n",
    "tokenizer.fit_on_texts(cleaned_tweets)\n",
    "word_index= tokenizer.word_index\n",
    "print(len(word_index))\n",
    "train_tokenized_sentence = tokenizer.texts_to_sequences(cleaned_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d079b50a-725c-4b5d-a71a-ec789aff96cc",
   "metadata": {},
   "source": [
    "# Padding the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e871e41-6ef5-4d9c-ad94-ef8649c75057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(seq, max_len = 45):\n",
    "    '''\n",
    "    Padding to make tweets same in length.\n",
    "    Filling empty spaces with 0.\n",
    "    '''\n",
    "    pad_value = 0\n",
    "    ls=[]\n",
    "    for i in seq:\n",
    "        pad_size = max_len - len(i)\n",
    "        final_list = [*i, *[pad_value] * pad_size]\n",
    "        ls.append(final_list)\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcda54e-88eb-42e2-b83c-97b5f8cb54aa",
   "metadata": {},
   "source": [
    "# Using the saved model and Testing with our test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cab83f49-2ddd-4487-a35a-b823160a667a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier(\n",
      "  (embedding): Embedding(35403, 100)\n",
      "  (LSTM): LSTM(100, 500, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=500, out_features=3, bias=True)\n",
      ")\n",
      "Accuracy on  twitter-dev-data.txt\n",
      "Length -  2000\n",
      "Accuracy on  twitter-dev-data.txt  is  0.4595\n",
      "Accuracy on  twitter-test1.txt\n",
      "Length -  3531\n",
      "Accuracy on  twitter-test1.txt  is  0.4259416595865194\n",
      "Accuracy on  twitter-test2.txt\n",
      "Length -  1853\n",
      "Accuracy on  twitter-test2.txt  is  0.36103615758229896\n",
      "Accuracy on  twitter-test3.txt\n",
      "Length -  2379\n",
      "Accuracy on  twitter-test3.txt  is  0.4131988230348886\n"
     ]
    }
   ],
   "source": [
    "from aspyfile import LSTMClassifier\n",
    "saved_model = torch.load('tushar.pth')\n",
    "saved_model.eval()\n",
    "print(saved_model)\n",
    "test_set = ['twitter-dev-data.txt','twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']\n",
    "test_data = read_file(test_set)\n",
    "for path in test_set:\n",
    "    print('Accuracy on ',path)\n",
    "    clean = cleanup_text(test_data[path]['tweet'])\n",
    "    clean_token = tokenizer.texts_to_sequences(clean)\n",
    "    padded_clean_token = torch.tensor(padding(clean_token)).to(device=device)\n",
    "    sent = torch.tensor(test_data[path]['sentiment']).to(device=device)\n",
    "    print('Length - ',len(padded_clean_token))\n",
    "    batch = 400\n",
    "    batch_iter = len(padded_clean_token)//batch \n",
    "    counter = 0\n",
    "    batch_counter = 1\n",
    "    flag = False\n",
    "    acc = 0\n",
    "    while True:\n",
    "        if(counter + batch >= len(sent)):\n",
    "            batch_text = padded_clean_token[counter:len(padded_clean_token)]\n",
    "            batch_sent = sent[counter:len(padded_clean_token)]\n",
    "            flag = True\n",
    "        else:\n",
    "            batch_text = padded_clean_token[counter:counter+batch]\n",
    "            batch_sent = sent[counter:counter+batch]\n",
    "        if(batch_counter % 25 == 0):\n",
    "            print('Calculating for batch ', batch_counter)\n",
    "        output = saved_model(batch_text)\n",
    "        for i in range(len(output)):\n",
    "            pred = torch.argmax(output[i]).to(device=device)\n",
    "            actual = torch.argmax(batch_sent[i]).to(device=device)\n",
    "            if (pred == actual):\n",
    "                acc = acc + 1\n",
    "        counter = counter + batch\n",
    "        batch_counter = batch_counter + 1\n",
    "        del batch_text, batch_sent, output, pred, actual\n",
    "        if (flag):\n",
    "            break\n",
    "    print('Accuracy on ',path,' is ', (acc/len(sent)))\n",
    "    # del clean, clean_token, padded_clean_token, sent, output, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb05b08-6846-4452-97bb-c4e052f349c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf74bd-30a3-47ef-8495-0319e4861f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
