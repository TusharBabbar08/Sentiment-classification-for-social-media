{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: This is a computational heavy task and might crash your browser\n",
    "# Importing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_list):\n",
    "    '''\n",
    "    Reads the txt file and assigns the parameters to respective list, updating the dictionary.\n",
    "    Also, performing One Hot Encoding on the Sentiments.\n",
    "    '''\n",
    "    dataset = {}\n",
    "    for path in file_list:\n",
    "        dataset[path] = {}\n",
    "        tweet = []\n",
    "        tweetgts = []\n",
    "        tweetid = []\n",
    "        with open(path, encoding='utf8') as file:\n",
    "            for line in file:\n",
    "                line = line[:len(line) - 1]\n",
    "                contents = line.split('\\t')\n",
    "                tweetid.append(int(contents[0]))\n",
    "                if(contents[1] == 'positive'):\n",
    "                    tweetgts.append([0, 1, 0])\n",
    "                elif(contents[1] == 'negative'):\n",
    "                    tweetgts.append([0, 0, 1])\n",
    "                else:\n",
    "                    tweetgts.append([1, 0, 0])\n",
    "                tweet.append(contents[2])\n",
    "        dataset[path]['tweet'] = tweet\n",
    "        dataset[path]['sentiment'] = tweetgts\n",
    "        dataset[path]['ids'] = tweetid\n",
    "    return dataset\n",
    "dataset = read_file(['twitter-training-data.txt', 'twitter-dev-data.txt','twitter-test1.txt','twitter-test2.txt','twitter-test3.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM will take a lot of time if running on cpu,\n",
    "## This checks if our machine has cuda cores or not.\n",
    "## Cuda can be enabled for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(texts):\n",
    "    '''\n",
    "    Pre-processed the tweets and returns a clean tweets after\n",
    "    replacing and removing the unwanted bits and pieces from the tweet.\n",
    "    '''\n",
    "    cleaned_text = []\n",
    "    for text in texts:\n",
    "        # remove ugly &quot and &amp\n",
    "        text = re.sub(r\"&quot;(.*?)&quot;\", \"\\g<1>\", text)\n",
    "        text = re.sub(r\"&amp;\", \"\", text)\n",
    "\n",
    "        # replace emoticon\n",
    "        text = re.sub(\n",
    "            r\"(^| )(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)\",\n",
    "            \"\\g<1>TOKEMOTICON\",\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"tokemoticon\", \"TOKEMOTICON\")\n",
    "\n",
    "        # replace url\n",
    "        text = re.sub(\n",
    "            r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\",\n",
    "            \"TOKURL\",\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        # replace mention\n",
    "        text = re.sub(r\"@[\\w]+\", \"TOKMENTION\", text)\n",
    "\n",
    "        # replace hashtag\n",
    "        text = re.sub(r\"#[\\w]+\", \"TOKTAG\", text)\n",
    "\n",
    "        # replace dollar\n",
    "        text = re.sub(r\"\\Â£\\d+\", \"TOKPOUND\", text)\n",
    "\n",
    "        # remove punctuation\n",
    "        text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "\n",
    "        # remove multiple spaces\n",
    "        text = re.sub(r\" +\", \" \", text)\n",
    "\n",
    "        # remove newline\n",
    "        text = re.sub(r\"\\n\", \" \", text)\n",
    "        \n",
    "        #Remove Digits\n",
    "        text= re.sub('[0-9\\n]',' ',text)\n",
    "\n",
    "        cleaned_text.append(text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use other test files for generating different validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35402\n"
     ]
    }
   ],
   "source": [
    "cleaned_tweets = cleanup_text(dataset['twitter-training-data.txt']['tweet'])\n",
    "v_clean_tweets = cleanup_text(dataset['twitter-dev-data.txt']['tweet'])\n",
    "tokenizer = Tokenizer(num_words = 5000,oov_token='<oov>')\n",
    "tokenizer.fit_on_texts(cleaned_tweets)\n",
    "word_index= tokenizer.word_index\n",
    "print(len(word_index))\n",
    "train_tokenized_sentence = tokenizer.texts_to_sequences(cleaned_tweets)\n",
    "valid_tokenized_sentence = tokenizer.texts_to_sequences(v_clean_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(seq, max_len = 45):\n",
    "    '''\n",
    "    Padding to make tweets same in length.\n",
    "    Filling empty spaces with 0.\n",
    "    '''\n",
    "    pad_value = 0\n",
    "    ls=[]\n",
    "    for i in seq:\n",
    "        pad_size = max_len - len(i)\n",
    "        final_list = [*i, *[pad_value] * pad_size]\n",
    "        ls.append(final_list)\n",
    "    return ls\n",
    "train_padded_seq = padding(train_tokenized_sentence)\n",
    "valid_padded_seq = padding(valid_tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet_tensor = torch.tensor(train_padded_seq).to(device=device)\n",
    "train_sentiment_tensor = torch.tensor(dataset['twitter-training-data.txt']['sentiment']).to(device=device)\n",
    "\n",
    "valid_tweet_tensor = torch.tensor(valid_padded_seq).to(device=device)\n",
    "valid_sentiment_tensor = torch.tensor(dataset['twitter-dev-data.txt']['sentiment']).to(device=device)\n",
    "\n",
    "train_tensor_dataset = TensorDataset(train_tweet_tensor, train_sentiment_tensor)\n",
    "valid_tensor_dataset = TensorDataset(valid_tweet_tensor, valid_sentiment_tensor)\n",
    "\n",
    "train_tensor_dataloader = DataLoader(train_tensor_dataset, batch_size=400, shuffle=True)\n",
    "valid_tensor_dataloader = DataLoader(valid_tensor_dataset, batch_size=400, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the GloVe embedding file and returning weight_matrix to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "\n",
    "with open('glove.6B.100d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = str(l)\n",
    "        line = line[2:len(line) - 3].split(' ')\n",
    "        val = [float(line[i]) for i in range(1, len(line))]\n",
    "        glove[line[0]] = val\n",
    "\n",
    "\n",
    "matrix_len = len(word_index) + 1\n",
    "weights_matrix = np.zeros((matrix_len, 100)) # 100 because this version of glove has 100 dims for a word\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(word_index):\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(100, ))\n",
    "weights_matrix = torch.tensor(weights_matrix).to(device=device)\n",
    "\n",
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embed, embed_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embed, embed_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    return emb_layer, num_embed, embed_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers, num_classes, dropout = 0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding, _, embedding_dim = create_emb_layer(weights_matrix, non_trainable=True)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, self.hidden_size, self.num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embed = self.embedding(input)\n",
    "        hidden = self.init_hidden(input.size(0))\n",
    "        out, _ = self.LSTM(embed, hidden)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return F.softmax(out, dim=1)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n",
    "                      weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5001\n",
    "embedding_dim = 100 \n",
    "hidden_size = 500\n",
    "num_classes = 3 \n",
    "num_layers = 2\n",
    "EPOCHS = 1\n",
    "lr = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "clip = 5\n",
    "BATCH_SIZE = 500\n",
    "model = LSTMClassifier(weights_matrix, hidden_size, num_layers, num_classes).to(device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1\n",
      "Epoch: 1/1... Step: 10... Loss: 0.621390... Val Loss: 0.629132\n",
      "Epoch: 1/1... Step: 20... Loss: 0.621878... Val Loss: 0.637801\n",
      "Epoch: 1/1... Step: 30... Loss: 0.614338... Val Loss: 0.611651\n",
      "Epoch: 1/1... Step: 40... Loss: 0.614044... Val Loss: 0.614745\n",
      "Epoch: 1/1... Step: 50... Loss: 0.624078... Val Loss: 0.608102\n",
      "Epoch: 1/1... Step: 60... Loss: 0.609772... Val Loss: 0.609923\n",
      "Epoch: 1/1... Step: 70... Loss: 0.605004... Val Loss: 0.611138\n",
      "Epoch: 1/1... Step: 80... Loss: 0.610643... Val Loss: 0.608772\n",
      "Epoch: 1/1... Step: 90... Loss: 0.623958... Val Loss: 0.609026\n",
      "Epoch: 1/1... Step: 100... Loss: 0.596491... Val Loss: 0.608315\n",
      "Epoch: 1/1... Step: 110... Loss: 0.620382... Val Loss: 0.611771\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    counter = 0\n",
    "    print('Epoch - '+str(i + 1))\n",
    "    for inputs, labels in train_tensor_dataloader:\n",
    "        counter += 1\n",
    "        model.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        if counter%10 == 0:\n",
    "            val_h = model.init_hidden(BATCH_SIZE)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in valid_tensor_dataloader:\n",
    "                out = model(inp)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, EPOCHS),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saves the Trained Model for testing Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'tushar.pth')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
