{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Two:  Sentiment Classification\n",
    "\n",
    "For this exercise you will be using the \"SemEval 2017 task 4\" corpus provided on the module website, available through the following link: https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs918/semeval-tweets.tar.bz2 You will focus particularly on Subtask A, i.e. classifying the overall sentiment of a tweet as positive, negative or neutral.\n",
    "\n",
    "You are requested to produce a standalone Python program or Jupyter notebook for coursework submission. The input to your program is the SemEval data downloaded. Note that TAs need to run your program on their own machine by using the original SemEval data. As such, donâ€™t submit a Python program that takes as input some preprocessed files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages\n",
    "You may import more packages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import re\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test sets\n",
    "testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    readin the testset and return a dictionary\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training set, dev set and testing set\n",
    "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_388/2006133427.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;31m#x_train and y_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweetgts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_388/2006133427.py\u001b[0m in \u001b[0;36mfeature_train\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[0mtfidf_vectoriser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectoriser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleanup_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_vectoriser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TF-IDF_Vocabulary.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Load training set, dev set and testing set\n",
    "data = {}\n",
    "tweetids = {}\n",
    "tweetgts = {}\n",
    "tweets = {}\n",
    "\n",
    "for dataset in ['twitter-training-data.txt'] + testsets:\n",
    "    data[dataset] = []\n",
    "    tweets[dataset] = []\n",
    "    tweetids[dataset] = []\n",
    "    tweetgts[dataset] = []\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def cleanup_text(texts):\n",
    "    cleaned_text = []\n",
    "    for text in texts:\n",
    "        # remove ugly &quot and &amp\n",
    "        text = re.sub(r\"&quot;(.*?)&quot;\", \"\\g<1>\", text)\n",
    "        text = re.sub(r\"&amp;\", \"\", text)\n",
    "\n",
    "        # replace emoticon\n",
    "        text = re.sub(\n",
    "            r\"(^| )(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)\",\n",
    "            \"\\g<1>TOKEMOTICON\",\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"tokemoticon\", \"TOKEMOTICON\")\n",
    "\n",
    "        # replace url\n",
    "        text = re.sub(\n",
    "            r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\",\n",
    "            \"TOKURL\",\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        # replace mention\n",
    "        text = re.sub(r\"@[\\w]+\", \"TOKMENTION\", text)\n",
    "\n",
    "        # replace hashtag\n",
    "        text = re.sub(r\"#[\\w]+\", \"TOKHASHTAG\", text)\n",
    "\n",
    "        # replace dollar\n",
    "        text = re.sub(r\"\\$\\d+\", \"TOKDOLLAR\", text)\n",
    "\n",
    "        # remove punctuation\n",
    "        text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "\n",
    "        # remove multiple spaces\n",
    "        text = re.sub(r\" +\", \" \", text)\n",
    "\n",
    "        # remove newline\n",
    "        text = re.sub(r\"\\n\", \" \", text)\n",
    "        \n",
    "        #Remove Digits\n",
    "        text= re.sub('[0-9\\n]',' ',text)\n",
    "\n",
    "        cleaned_text.append(text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "            \n",
    "# #function for pre-processing the data\n",
    "# def pre_process_data(senti_data):\n",
    "    \n",
    "  \n",
    "#     senti_data['processed_text'] = senti_data['tweets']\n",
    "#    # calling all the functions for pre-processing that were defined earlier\n",
    "#     senti_data['processed_text'] = senti_data['processed_text'].apply(lambda x : x.lower())\n",
    "#     senti_data['processed_text'] = np.vectorize(spec_punc_remove)(senti_data['processed_text'])\n",
    "#     senti_data['processed_text'] = np.vectorize(contract_convert)(senti_data['processed_text'])\n",
    "#     senti_data['processed_text'] = np.vectorize(remove_single_char)(senti_data['processed_text'])\n",
    "    \n",
    "#     senti_data['processed_text'] = np.vectorize(remove_space)(senti_data['processed_text'])\n",
    "\n",
    "#     senti_data['processed_text'] = np.vectorize(replace_url)(senti_data['processed_text'])\n",
    "    \n",
    "#     senti_data['processed_text'] = np.vectorize(replace_mentions)(senti_data['processed_text'])\n",
    "#     senti_data['processed_text'] = np.vectorize(remove_numbers)(senti_data['processed_text'])\n",
    "\n",
    "#     senti_data['processed_text'] = np.vectorize(replace_hashtags)(senti_data['processed_text'])      \n",
    "\n",
    "#     senti_data['processed_text'] = np.vectorize(remove_punct)(senti_data['processed_text'])       \n",
    "#     senti_data['processed_text'] = senti_data['processed_text'].str.strip()    \n",
    "#     senti_data['processed_text'] = np.vectorize(replace_Long)(senti_data['processed_text'])  \n",
    "#     senti_data['processed_text'] = np.vectorize(remove_digit)(senti_data['processed_text']) \n",
    "#     #Adding exta features namely, positive, negative , and Abusive words\n",
    "#     senti_data['Abusive words'] = senti_data['processed_text'].apply(lambda x: len(re.findall(abs_words_pattern,x)))\n",
    "#     senti_data['positive words'] = senti_data['processed_text'].apply(lambda x: len(re.findall(pos_words_pattern,x)))\n",
    "#     senti_data['negative words'] = senti_data['processed_text'].apply(lambda x: len(re.findall(neg_words_pattern,x)))\n",
    "    \n",
    "    \n",
    "#     return senti_data \n",
    "# # write code to read in the datasets here\n",
    "\n",
    "#Loading the training data as Dictionary\n",
    "data = {}\n",
    "with open('twitter-training-data.txt', 'r', encoding='utf-8') as file:\n",
    "     for lines in file:\n",
    "            col_val = lines.split('\\t')\n",
    "            tweetids = col_val[0]\n",
    "            tweetgts = col_val[1]\n",
    "            tweets = col_val[2].strip()\n",
    "            data[tweetids] = tweetgts, tweets \n",
    "#Converting dic file to dataframe\n",
    "data = pd.DataFrame.from_dict(data,orient='index',columns = ['tweetgts','tweets'])\n",
    "data['tweetids'] = data.index\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "#calling the function for pre-processing the training data\n",
    "data = cleanup_text(data)\n",
    "\n",
    "#feature extraction using Tf-idf for test data\n",
    "def feature_test(data, vocab):\n",
    "    tfidf_vectoriser = TfidfVectorizer(min_df=5, ngram_range=(1,3), vocabulary=vocab)\n",
    "  \n",
    "    tfidf = tfidf_vectoriser.fit_transform(cleanup_text(data['tweets']))\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "#feature extraction using tf-idf for train data\n",
    "def feature_train(data):\n",
    "    tfidf_vectoriser = TfidfVectorizer(min_df=5, ngram_range=(1,3))\n",
    "\n",
    "    tfidf = tfidf_vectoriser.fit_transform(cleanup_text(data['tweets']))\n",
    "\n",
    "    pickle.dump(tfidf_vectoriser.vocabulary_, open('TF-IDF_Vocabulary.pkl', 'wb'))\n",
    "\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "#x_train and y_train \n",
    "x_train = feature_train(data)\n",
    "y_train = data['tweetgts']\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(x_train, y_train)\n",
    "pickle.dump(logistic_regression, open('LogisticRegression.pkl', 'wb'))\n",
    "\n",
    "#Support vector machine\n",
    "SVM = LinearSVC()\n",
    "SVM.fit(x_train, y_train)\n",
    "pickle.dump(SVM, open('SVM.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sentiment classifiers\n",
    "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Support Vector Machine\n",
      "Training Support Vector Machine\n",
      "            positive  negative  neutral\n",
      "positive    0.697     0.059     0.244     \n",
      "negative    0.151     0.639     0.210     \n",
      "neutral     0.243     0.149     0.608     \n",
      "\n",
      "twitter-test1.txt (Support Vector Machine): 0.577\n",
      "            positive  negative  neutral\n",
      "positive    0.759     0.037     0.204     \n",
      "negative    0.157     0.672     0.172     \n",
      "neutral     0.333     0.097     0.571     \n",
      "\n",
      "twitter-test2.txt (Support Vector Machine): 0.634\n",
      "            positive  negative  neutral\n",
      "positive    0.733     0.056     0.211     \n",
      "negative    0.203     0.554     0.243     \n",
      "neutral     0.292     0.136     0.572     \n",
      "\n",
      "twitter-test3.txt (Support Vector Machine): 0.550\n",
      "Training Logistic Regression\n",
      "Training Logistic Regression\n",
      "            positive  negative  neutral\n",
      "positive    0.739     0.054     0.207     \n",
      "negative    0.164     0.735     0.102     \n",
      "neutral     0.229     0.161     0.610     \n",
      "\n",
      "twitter-test1.txt (Logistic Regression): 0.563\n",
      "            positive  negative  neutral\n",
      "positive    0.772     0.043     0.185     \n",
      "negative    0.118     0.720     0.161     \n",
      "neutral     0.339     0.109     0.552     \n",
      "\n",
      "twitter-test2.txt (Logistic Regression): 0.589\n",
      "            positive  negative  neutral\n",
      "positive    0.756     0.056     0.189     \n",
      "negative    0.170     0.678     0.152     \n",
      "neutral     0.294     0.142     0.564     \n",
      "\n",
      "twitter-test3.txt (Logistic Regression): 0.540\n"
     ]
    }
   ],
   "source": [
    "# Buid traditional sentiment classifiers. An example classifier name 'svm' is given\n",
    "# in the code below. You should replace the other two classifier names\n",
    "# with your own choices. For features used for classifier training, \n",
    "# the 'bow' feature is given in the code. But you could also explore the \n",
    "# use of other features.\n",
    "for classifier in ['Support Vector Machine', 'Logistic Regression']:\n",
    "    for features in ['bow', '<feature-2-name>']:\n",
    "        # Skeleton: Creation and training of the classifiers\n",
    "        if classifier == 'Support Vector Machine':\n",
    "            print('Training ' + classifier)\n",
    "            model = pickle.load(open('SVM.pkl','rb'))\n",
    "            vocab = pickle.load(open('TF-IDF_Vocabulary.pkl','rb'))\n",
    "        elif classifier == 'Logistic Regression':\n",
    "            print('Training ' + classifier)\n",
    "            model = pickle.load(open('LogisticRegression.pkl','rb'))\n",
    "            vocab = pickle.load(open('TF-IDF_Vocabulary.pkl','rb'))\n",
    "        else:\n",
    "            print('Unknown classifier name' + classifier)\n",
    "            continue\n",
    "\n",
    "        # Predition performance of the classifiers\n",
    "    for testset in testsets:\n",
    "        id_preds = {}\n",
    "        if(classifier == \"Support Vector Machine\") | (classifier == \"Logistic Regression\") :\n",
    "            #write the prediction and the evaluation code here\n",
    "            testset_name = testset\n",
    "            testset_path = join('semeval-tweets', testset_name)\n",
    "            #loading test data and calling feature_test function for feature extraction on the test data\n",
    "            data_test = pd.read_csv(testset, sep=\"\\t\", names=['tweetids', 'tweetgts', 'tweets'], dtype={'tweetids':'object','tweetgts':'object','tweets':'object'})                \n",
    "            data_test = pre_process_data(data_test)\n",
    "     \n",
    "            features_extract = feature_test(data_test, vocab)\n",
    "\n",
    "        \n",
    "            id_preds = dict(zip(data_test['tweetids'],model.predict(features_extract)))\n",
    "            confusion(id_preds,testset, classifier)\n",
    "            evaluate(id_preds, testset, classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
