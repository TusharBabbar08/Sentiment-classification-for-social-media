{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_list):\n",
    "    '''\n",
    "    Reads the txt file and assigns the parameters to respective list, updating the dictionary.\n",
    "    Also, performing One Hot Encoding on the Sentiments.\n",
    "    '''\n",
    "    dataset = {}\n",
    "    for path in file_list:\n",
    "        dataset[path] = {}\n",
    "        tweet = []\n",
    "        tweetgts = []\n",
    "        tweetid = []\n",
    "        with open(path, encoding='utf8') as file:\n",
    "            for line in file:\n",
    "                line = line[:len(line) - 1]\n",
    "                contents = line.split('\\t')\n",
    "                tweetid.append(int(contents[0]))\n",
    "                if(contents[1] == 'positive'):\n",
    "                    tweetgts.append([0, 1, 0])\n",
    "                elif(contents[1] == 'negative'):\n",
    "                    tweetgts.append([0, 0, 1])\n",
    "                else:\n",
    "                    tweetgts.append([1, 0, 0])\n",
    "                tweet.append(contents[2])\n",
    "        dataset[path]['tweet'] = tweet\n",
    "        dataset[path]['sentiment'] = tweetgts\n",
    "        dataset[path]['ids'] = tweetid\n",
    "    return dataset\n",
    "dataset = read_file(['twitter-training-data.txt', 'twitter-dev-data.txt','twitter-test1.txt','twitter-test2.txt','twitter-test3.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(texts):\n",
    "    '''\n",
    "    Pre-processed the tweets and returns a clean tweets after\n",
    "    replacing and removing the unwanted bits and pieces from the tweet.\n",
    "    '''\n",
    "    cleaned_text = []\n",
    "    for text in texts:\n",
    "        # remove ugly &quot and &amp\n",
    "        text = re.sub(r\"&quot;(.*?)&quot;\", \"\\g<1>\", text)\n",
    "        text = re.sub(r\"&amp;\", \"\", text)\n",
    "\n",
    "        # replace emoticon\n",
    "        text = re.sub(\n",
    "            r\"(^| )(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)\",\n",
    "            \"\\g<1>TOKEMOTICON\",\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"tokemoticon\", \"TOKEMOTICON\")\n",
    "\n",
    "        # replace url\n",
    "        text = re.sub(\n",
    "            r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\",\n",
    "            \"TOKURL\",\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        # replace mention\n",
    "        text = re.sub(r\"@[\\w]+\", \"TOKMENTION\", text)\n",
    "\n",
    "        # replace hashtag\n",
    "        text = re.sub(r\"#[\\w]+\", \"TOKTAG\", text)\n",
    "\n",
    "        # replace dollar\n",
    "        text = re.sub(r\"\\Â£\\d+\", \"TOKPOUND\", text)\n",
    "\n",
    "        # remove punctuation\n",
    "        text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "\n",
    "        # remove multiple spaces\n",
    "        text = re.sub(r\" +\", \" \", text)\n",
    "\n",
    "        # remove newline\n",
    "        text = re.sub(r\"\\n\", \" \", text)\n",
    "        \n",
    "        #Remove Digits\n",
    "        text= re.sub('[0-9\\n]',' ',text)\n",
    "\n",
    "        cleaned_text.append(text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the tweets\n",
    "\n",
    "## Please change the test sets value below to get the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35402\n"
     ]
    }
   ],
   "source": [
    "cleaned_tweets = cleanup_text(dataset['twitter-training-data.txt']['tweet'])\n",
    "v_clean_tweets = cleanup_text(dataset['twitter-test3.txt']['tweet'])\n",
    "tokenizer = Tokenizer(num_words = 5000,oov_token='<oov>')\n",
    "tokenizer.fit_on_texts(cleaned_tweets)\n",
    "word_index= tokenizer.word_index\n",
    "print(len(word_index))\n",
    "train_tokenized_sentence = tokenizer.texts_to_sequences(cleaned_tweets)\n",
    "valid_tokenized_sentence = tokenizer.texts_to_sequences(v_clean_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(seq, max_len = 45):\n",
    "    '''\n",
    "    Padding to make tweets same in length.\n",
    "    Filling empty spaces with 0.\n",
    "    '''\n",
    "    pad_value = 0\n",
    "    ls=[]\n",
    "    for i in seq:\n",
    "        pad_size = max_len - len(i)\n",
    "        final_list = [*i, *[pad_value] * pad_size]\n",
    "        ls.append(final_list)\n",
    "    return ls\n",
    "train_padded_seq = padding(train_tokenized_sentence)\n",
    "valid_padded_seq = padding(valid_tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to Numpy Array, easier to feed to the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet = np.array(train_padded_seq)\n",
    "train_sentiment = np.array(np.argmax(dataset['twitter-training-data.txt']['sentiment'], axis=1))\n",
    "\n",
    "valid_tweet = np.array(valid_padded_seq)\n",
    "valid_sentiment = np.array(np.argmax(dataset['twitter-test3.txt']['sentiment'],axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(gamma='auto')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "def accuracy(y_pred, y_actual):\n",
    "    acc = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if (y_pred[i] == y_actual[i]):\n",
    "            acc = acc + 1\n",
    "    acc = (acc/len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "svc = SVC(gamma='auto')\n",
    "svc.fit(train_tweet, train_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine (SVM) accuracy on test data - 41.40395124001681 %\n"
     ]
    }
   ],
   "source": [
    "y_pred = svc.predict(valid_tweet)\n",
    "print('Support Vector Machine (SVM) accuracy on test data - ' + str(accuracy(y_pred, valid_sentiment)) + ' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
