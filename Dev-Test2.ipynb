{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d75898-799f-429e-b1ae-f00a20c96081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce111e95-5ea2-4f92-8d5d-359b0d1c0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "308a661d-23d2-4adb-bc88-7862e7521b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test sets\n",
    "testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8505314-8641-4aa6-9060-20907edf90e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    readin the testset and return a dictionary\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449c8e1f-6b7b-41ba-914b-6c36658bf6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set, dev set and testing set\n",
    "data = {}\n",
    "tweetids = {}\n",
    "tweetgts = {}\n",
    "tweets = {}\n",
    "\n",
    "for dataset in ['twitter-training-data.txt'] + testsets:\n",
    "    data[dataset] = []\n",
    "    tweets[dataset] = []\n",
    "    tweetids[dataset] = []\n",
    "    tweetgts[dataset] = []\n",
    "    \n",
    "\n",
    "\n",
    "    # write code to read in the datasets here\n",
    "    with open(dataset, encoding='utf8') as file:\n",
    "        for check in file:\n",
    "            senti=check.split('\\t')\n",
    "            tweetids[dataset].append(senti[0]) \n",
    "            tweetgts[dataset].append(senti[1]) \n",
    "            tweets[dataset].append(senti[2])\n",
    "# print(len(tweetids)) \n",
    "\n",
    "#     for i in range(len(tweets['twitter-training-data.txt'])):\n",
    "#         tweets[i]=re.sub(r\"https*://[^\\s]+\",\"\",tweets[i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d7e77d-d3dc-44f7-bd1f-8104b108b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def cleanup_text(texts):\n",
    "    cleaned_text = []\n",
    "    for text in texts:\n",
    "        # remove ugly &quot and &amp\n",
    "        text = re.sub(r\"&quot;(.*?)&quot;\", \"\\g<1>\", text)\n",
    "        text = re.sub(r\"&amp;\", \"\", text)\n",
    "\n",
    "        # replace emoticon\n",
    "        text = re.sub(\n",
    "            r\"(^| )(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)\",\n",
    "            \"\\g<1>TOKEMOTICON\",\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"tokemoticon\", \"TOKEMOTICON\")\n",
    "\n",
    "        # replace url\n",
    "        text = re.sub(\n",
    "            r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\",\n",
    "            \"TOKURL\",\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        # replace mention\n",
    "        text = re.sub(r\"@[\\w]+\", \"TOKMENTION\", text)\n",
    "\n",
    "        # replace hashtag\n",
    "        text = re.sub(r\"#[\\w]+\", \"TOKHASHTAG\", text)\n",
    "\n",
    "        # replace dollar\n",
    "        text = re.sub(r\"\\$\\d+\", \"TOKDOLLAR\", text)\n",
    "\n",
    "        # remove punctuation\n",
    "        text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "\n",
    "        # remove multiple spaces\n",
    "        text = re.sub(r\" +\", \" \", text)\n",
    "\n",
    "        # remove newline\n",
    "        text = re.sub(r\"\\n\", \" \", text)\n",
    "        \n",
    "        #Remove Digits\n",
    "        text= re.sub('[0-9\\n]',' ',text)\n",
    "\n",
    "        cleaned_text.append(text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6479ea9-1e71-4378-ada0-0b5eb99de118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tusha\\anaconda3\\envs\\Term1NLP\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['felt privileged play foo fighters songs guitar today plectrums gig saturday']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "encoded=[]\n",
    "for t in cleanup_text(tweets['twitter-training-data.txt']):\n",
    "    filtered=remove_stopwords(t)\n",
    "    encoded.append(filtered.lower())\n",
    "#     t=t.split(' ')\n",
    "#     t=[w for w in t if (w not in stop)]\n",
    "#     encoded.append(tokenizer.texts_to_sequences(t))\n",
    "print(encoded[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef2a632d-fb6e-4e61-9611-7421f744cb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35091\n",
      "[[1973, 1, 64, 213, 192, 624, 2594, 23, 1, 1606, 20], [2, 1461, 2377, 366, 1, 295, 260, 148, 1074, 34, 366, 1], [47, 73, 4374, 4375, 1, 2, 27, 1, 1, 1699, 387, 1, 164, 996, 1438], [2, 1, 11, 4376, 7, 5, 33, 27], [1, 3, 3271, 14, 1133, 495, 3733, 4]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=5000,oov_token='<oov>')\n",
    "tokenizer.fit_on_texts(encoded)\n",
    "\n",
    "word_index= tokenizer.word_index\n",
    "print(len(word_index))\n",
    "\n",
    "seq=tokenizer.texts_to_sequences(encoded)\n",
    "print(seq[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb9e5fb7-c37b-4698-8e64-bdcf1270c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment={'positive':1,\n",
    "           'neutral':0,\n",
    "           'negative':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b74d408b-ecf7-4edf-a850-72fb1e9b22e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 2, 2, 1, 1, 1, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "senti=[]\n",
    "for i in tweetgts['twitter-training-data.txt']:\n",
    "    # print(i)\n",
    "    senti.append(sentiment[i])\n",
    "print(senti[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da6758a9-6e05-4160-9463-1deea174d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "senti=[]\n",
    "for i in tweetgts['twitter-training-data.txt']:\n",
    "    senti.append(sentiment[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09bdc0dd-9e6a-4840-8c32-b8c31d696b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1132, 1, 1, 1, 1, 1, 136, 1693, 122, 1, 4452, 8, 1, 1], [491, 1, 1, 626, 1, 1, 1, 656, 1, 1, 121, 1, 1, 1, 1, 1, 49, 1, 1304], [60, 1624, 667, 1, 537, 1, 1, 41], [1, 1, 1, 2859, 1, 1, 1, 1, 1, 2679, 1211, 1, 1, 1, 1, 1, 1, 858, 1, 46, 1, 2494], [1, 33, 1, 1945, 482, 3555, 1, 1, 1, 1031, 1, 18, 45, 1, 1, 232, 1, 4401, 1, 4515, 4452, 8, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_seq=tokenizer.texts_to_sequences(tweets[\"twitter-test1.txt\"])\n",
    "print(test_seq[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce162426-5905-44b2-ba91-d95f88d2b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "\n",
    "N = 40\n",
    "\n",
    "pad_value = 0\n",
    "ls=[]\n",
    "for i in seq:\n",
    "    pad_size = N - len(i)\n",
    "\n",
    "    final_list = [*i, *[pad_value] * pad_size]\n",
    "    ls.append(final_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb910744-557d-4146-bc0f-36757012aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ls:\n",
    "    if(len(i)>40 or len(i)<40):\n",
    "        print(i,' +', len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a86d2ac0-9bf8-4f08-bbcc-7aeead42a33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1461, 2377, 366, 1, 295, 260, 148, 1074, 34, 366, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(ls[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a950cd6f-f87f-4dd2-a8ad-ab54a4e6511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(ls)\n",
    "y=torch.tensor(senti)\n",
    "\n",
    "abc=TensorDataset(x,y)\n",
    "batch_size=500\n",
    "load=DataLoader(abc,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc08f98d-b83f-41f2-9f7c-838c8bf44b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35092\n"
     ]
    }
   ],
   "source": [
    "# Building Glove\n",
    "\n",
    "glove = {}\n",
    "\n",
    "with open('glove.6B.100d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = str(l)\n",
    "        line = line[2:len(line) - 3].split(' ')\n",
    "        val = [float(line[i]) for i in range(1, len(line))]\n",
    "        glove[line[0]] = val\n",
    "\n",
    "\n",
    "matrix_len = len(word_index) + 1\n",
    "weights_matrix = np.zeros((matrix_len, 100))\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(word_index):\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(100, ))\n",
    "weights_matrix = torch.tensor(weights_matrix)\n",
    "\n",
    "print(len(weights_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4dbcc575-9c2c-41ae-a4da-439581af5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING\n",
    "\n",
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers, num_classes, dropout = 0.25):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_sweights_matrixidden_size\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, non_trainable=True)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, self.hidden_size, self.num_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense = nn.Linear(hidden_size, 256)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        hidden = self.init_hidden(x.size(0))\n",
    "        out, _ = self.LSTM(embed, hidden)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.dense(out))\n",
    "        out = self.fc(out)\n",
    "        return torch.exp(F.log_softmax(out, dim=1))\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n",
    "                      weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4900fe1a-c23c-4859-8a79-ab2a39d314d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LSTMClassifier' object has no attribute 'hidden_sweights_matrixidden_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4844/4236105886.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mclip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTMClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4844/3200310198.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, weights_matrix, hidden_size, num_layers, num_classes, dropout)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTMClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_sweights_matrixidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_emb_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_trainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Term1NLP\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1130\u001b[1;33m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m   1131\u001b[0m             type(self).__name__, name))\n\u001b[0;32m   1132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LSTMClassifier' object has no attribute 'hidden_sweights_matrixidden_size'"
     ]
    }
   ],
   "source": [
    "## TRAINING THE LSTM\n",
    "\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_dim = 100 # Since we have choosen glove embedding 100 dimenstion version\n",
    "hidden_size = 500\n",
    "num_classes = 3\n",
    "num_epochs = 2\n",
    "num_layers = 2\n",
    "EPOCHS = 10\n",
    "lr=0.001\n",
    "criterion = nn.BCELoss()\n",
    "clip = 5\n",
    "\n",
    "net = LSTMClassifier(weights_matrix, hidden_size, num_layers, num_classes)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "input = torch.randint(1, 4, (5, 3))\n",
    "output = net(input)\n",
    "net.train()\n",
    "for i in range(EPOCHS):\n",
    "    counter = 0\n",
    "    print('Epoch - '+str(i + 1))\n",
    "    for inputs, labels in enumerate(word_index):\n",
    "        counter += 1\n",
    "        net.zero_grad()\n",
    "        # print(inputs.size())\n",
    "        output = net(inputs)\n",
    "        # print(labels.float())\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%5 == 0:\n",
    "            val_h = net.init_hidden(BATCH_SIZE)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inp, lab in valid_dataloader:\n",
    "                # val_h = tuple([each.data for each in val_h])\n",
    "                out = net(inp)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, EPOCHS),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d7c3f9-a53b-4fba-ad86-6871f6b561e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b50e83-a312-4995-b904-559d659ff117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0961fa83-2f79-49fd-85a0-9a17d66fb828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f3e13-c069-474f-a3e4-8eafda001648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bcc5be-01ee-45c9-a0d7-f749dbfae13b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
